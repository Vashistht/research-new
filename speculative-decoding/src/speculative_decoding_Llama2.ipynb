{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speculative Decoding using \n",
    "- GPTQ: https://arxiv.org/abs/2210.17323\n",
    "- https://huggingface.co/docs/transformers/v4.34.0/main_classes/quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import Tuple\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from transformers import set_seed\n",
    "import os \n",
    "import dotenv\n",
    "from speculative_sampling_helper import *\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HFI API retrieved succeessfully:  True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "# dotenv.load_dotenv('/Users/vashisth/Desktop/research-new/speculative-decoding/.env')\n",
    "dotenv.load_dotenv('/home/vashistt/research-new/speculative-decoding/.env')\n",
    "\n",
    "hf_api_key = os.getenv('hf_api') \n",
    "print('HFI API retrieved succeessfully: ', os.getenv('hf_api') is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accelerator = Accelerator()\n",
    "# device = accelerator.device\n",
    "# device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TextStreamer, GPTQConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the models, where you want to save the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models used here\n",
    "- Target: meta-llama/Llama-2-7b-chat\n",
    "- Quantised:\n",
    "  - TheBloke/Llama-2-7b-Chat-GPTQ\n",
    "  - https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF\n",
    "- Pruned: https://huggingface.co/princeton-nlp/Sheared-LLaMA-2.7B-ShareGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = 'Llama2_(4bit)quantized'\n",
    "# path_dir = '../results/llama2'\n",
    "path_dir = '/home/vashistt/research-new/speculative-decoding/results/llama2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/vashisth/Desktop/research-new/speculative-decoding/src', '/Users/vashisth/anaconda3/envs/speculative_decoding/lib/python311.zip', '/Users/vashisth/anaconda3/envs/speculative_decoding/lib/python3.11', '/Users/vashisth/anaconda3/envs/speculative_decoding/lib/python3.11/lib-dynload', '', '/Users/vashisth/anaconda3/envs/speculative_decoding/lib/python3.11/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "print(sys.path[0])\n",
    "\n",
    "df_test = pd.read_csv('sampling-times-temp0.csv')\n",
    "temperature = 0\n",
    "csv_file_path = f'{path_dir}/{model_names}-TEST_sampling-times-temp{temperature}.csv'\n",
    "df_test.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "gptq_path  = 'TheBloke/Llama-2-7b-Chat-GPTQ' \n",
    "pruned_path = \"princeton-nlp/Sheared-LLaMA-2.7B-ShareGPT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token = hf_api_key) # tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the model configuration for quantized models\n",
    "gptq_config_4 = GPTQConfig(bits=4, tokenizer=tokenizer,dataset=\"wikitext2\", group_size = 64, desc_act = True, damp_percent = 0.01) # using gptq-4bit-64g-actorder_True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading target model\n",
    "\n",
    "# llm = AutoModelForCausalLM.from_pretrained(\"TheBloke/Mistral-7B-v0.1-GGUF\", model_file=\"mistral-7b-v0.1.Q4_K_M.gguf\", model_type=\"mistral\", gpu_layers=50, hf=-1)\n",
    "# target_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\",load_in_8bit=True)\n",
    "target_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", token = hf_api_key)\n",
    "# ,torch_dtype=torch.bfloat16)\n",
    "# .to(device)\n",
    "target_generator = pipeline('text-generation', model=target_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid input\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vashisth/anaconda3/envs/speculative_decoding/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# loading draft model\n",
    "# have the user enter to decide which draft model to use \n",
    "user_input = input(\"Enter the draft model to use: 1. For Pruned \\n 2. For Quantized\")\n",
    "\n",
    "if user_input == \"1\":\n",
    "    draft_model = AutoModelForCausalLM.from_pretrained(pruned_path, device_map=\"auto\", token = hf_api_key)\n",
    "    draft_generator = pipeline('text-generation', model=draft_model, tokenizer=tokenizer)\n",
    "elif user_input == '2':\n",
    "    draft_model = AutoModelForCausalLM.from_pretrained(gptq_path, quantization_config=gptq_config_4, device_map=\"auto\", token = hf_api_key)\n",
    "    draft_generator = pipeline('text-generation', model=draft_model, tokenizer=tokenizer)\n",
    "else:\n",
    "    print(\"Invalid input\")\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draft_model = AutoModelForCausalLM.from_pretrained(pruned_path, device_map=\"auto\",token = hf_api_key)\n",
    "# draft_generator= pipeline('text-generation', model=draft_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'The quick brown fox'\n",
    "inputs = tokenizer(question, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = speculative_sampling(target_model, draft_model, initial_prompt_seq=inputs.input_ids, max_new_tokens= 15, lookahead=4, tokenizer=tokenizer, temperature=0., debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokens = len(tokens[0]) - len(inputs.input_ids[0])\n",
    "print(new_tokens)\n",
    "print(tokenizer.decode(tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = autoregressive_sampling(target_model, initial_prompt_seq=inputs.input_ids, max_new_tokens=15, temperature=0.)\n",
    "new_tokens = len(tokens[0]) - len(inputs.input_ids[0])\n",
    "print(new_tokens)\n",
    "print(tokenizer.decode(tokens[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'For polynomial regression, which one of these structural assumptions is the one that most affects the trade-off between underfitting and overfitting:\t[ \"The polynomial degree\", \"Whether we learn the weights by matrix inversion or gradient descent\", \"The assumed variance of the Gaussian noise\", \"The use of a constant-term unit input\" ]. Answer and explain.'\n",
    "temperature = 0\n",
    "max_lengths = [32, 64, 128, 256]  # example max_lengths\n",
    "lookahead_ks = [1, 2, 3, 4, 5, 7, 8]  # example lookahead_ks\n",
    "\n",
    "# Store the results\n",
    "results = []\n",
    "\n",
    "# Loop over different max_length and k values\n",
    "for max_length in max_lengths:\n",
    "    # Run autoregressive sampling\n",
    "    output_ar, time_ar, new_tokens_ar = sampling_test(prompt, tokenizer, 'autoregressive', target_model, draft_model, max_new_tokens=max_length)\n",
    "    \n",
    "    results.append(['autoregressive', max_length, 0, time_ar, output_ar, new_tokens_ar])\n",
    "    \n",
    "    for k in lookahead_ks:\n",
    "        # Run speculative sampling\n",
    "        output_sp, time_sp, new_tokens_sp = sampling_test(prompt, tokenizer, 'speculative', target_model, draft_model, max_new_tokens=max_length, lookahead_k=k)\n",
    "\n",
    "        results.append(['speculative', max_length, k, time_sp, output_sp, new_tokens_sp])\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(results, columns=['Sampling Method', 'Max Length', 'K Values', 'Time Taken', 'Text Generated', 'New Tokens'])\n",
    "\n",
    "# Save the results to a CSV file\n",
    "csv_file_path = f'{path_dir}/{model_names}-sampling-times-temp{temperature}.csv'\n",
    "df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the unique N values (Max Length) and K values for lookahead\n",
    "max_lengths_sorted = sorted(df['Max Length'].unique())\n",
    "lookahead_ks = sorted(df[df['Sampling Method'] == 'speculative']['K Values'].unique())\n",
    "ind = np.arange(len(max_lengths_sorted))\n",
    "bar_width = 0.1\n",
    "\n",
    "\n",
    "# Adjust the plot aesthetics as per the user's request\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Set the color for the bars\n",
    "color_speculative = 'lightgreen'  # A more pleasant color for speculative bars\n",
    "color_autoregressive = 'lightblue'  # A more pleasant color for autoregressive bars\n",
    "\n",
    "# Iterate over max_lengths and k_values to create bar positions\n",
    "for i, max_length in enumerate(max_lengths_sorted):\n",
    "    # Select subset of DataFrame for the current N (Max Length)\n",
    "    subset = df[df['Max Length'] == max_length]\n",
    "    \n",
    "    # Plot bars for autoregressive method\n",
    "    autoregressive_time = subset[subset['Sampling Method'] == 'autoregressive']['Time Taken'].values\n",
    "    if autoregressive_time.size > 0:\n",
    "        ax.bar(ind[i] - bar_width/2, autoregressive_time, bar_width, label='Autoregressive' if i == 0 else \"\", \n",
    "               color=color_autoregressive, edgecolor='black')\n",
    "    \n",
    "    # Plot bars for speculative method with different K values\n",
    "    speculative_subset = subset[subset['Sampling Method'] == 'speculative']\n",
    "    for j, k_value in enumerate(speculative_subset['K Values']):\n",
    "        time = speculative_subset[speculative_subset['K Values'] == k_value]['Time Taken'].values\n",
    "        if time.size > 0:\n",
    "            bar = ax.bar(ind[i] + (j+0.5)*bar_width, time, bar_width, label=f'Speculative K' if i == 0 else \"\", \n",
    "                         color=color_speculative, edgecolor='black')\n",
    "            # Annotate K value on the bar\n",
    "            ax.annotate(f'K={k_value}',\n",
    "                        xy=(bar[0].get_x() + bar[0].get_width() / 2, bar[0].get_height()),\n",
    "                        xytext=(0, 3),  # 3 points vertical offset\n",
    "                        textcoords=\"offset points\",\n",
    "                        ha='center', va='bottom')\n",
    "\n",
    "# Add grid to the plot with reduced alpha for less bold lines\n",
    "ax.yaxis.grid(True, linestyle='--', which='major', color='grey', alpha=0.5)\n",
    "\n",
    "# Set the labels and legend\n",
    "ax.set_xticks(ind)\n",
    "ax.set_xticklabels(max_lengths_sorted)\n",
    "ax.set_xlabel('Max Length (N)')\n",
    "ax.set_ylabel('Time Taken (seconds)')\n",
    "ax.set_title(f'Time Taken for Different Sampling Methods and K Values (temp = {temperature})')\n",
    "ax.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot to a file\n",
    "plot_file_path = f'{path_dir}/{model_names}-bar-plot-sampling-times-temp{temperature}.png'\n",
    "plt.savefig(plot_file_path)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lengths_sorted = sorted(df['Max Length'].unique())\n",
    "\n",
    "for max_length in max_lengths_sorted:\n",
    "    k = df[df['Max Length'] == max_length]['K Values']\n",
    "    time_list = df[df['Max Length'] == max_length]['Time Taken'] * 1000\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(k, time_list, label=f\"Max Length = {max_length}\", linestyle='--', marker='x')\n",
    "    plt.plot(k, time_list, 'x', color='red')\n",
    "\n",
    "    plt.grid(True, axis='y', linestyle='--', which='major', color='grey', alpha=0.5)\n",
    "    plt.xlabel('Number of draft tokens (K)')\n",
    "    plt.ylabel('Time Taken (m seconds)')\n",
    "    plt.title(f'Time Taken for K Values for max-length = {max_length} (temp = {temperature})')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{path_dir}/{model_names}-time-taken-max-length-{max_length}.png')\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speculative_decoding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
