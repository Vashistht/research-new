{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speculative Decoding using \n",
    "- GPTQ: https://arxiv.org/abs/2210.17323\n",
    "- https://huggingface.co/docs/transformers/v4.34.0/main_classes/quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import Tuple\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from transformers import set_seed\n",
    "import os \n",
    "import dotenv\n",
    "from speculative_sampling_helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TextStreamer, GPTQConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = 'Mistral-16vs4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gptq_config_2 = GPTQConfig(bits=2, tokenizer=tokenizer)\n",
    "# quantization_4 = GPTQConfig(bits=4, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gptq_path = 'TheBloke/Mistral-7B-v0.1-GPTQ'\n",
    "\n",
    "gptq_config_4 = GPTQConfig(bits=4, tokenizer=tokenizer,dataset=\"c4\")\n",
    "draft_model = AutoModelForCausalLM.from_pretrained(gptq_path, quantization_config=gptq_config_4, device_map=\"auto\")\n",
    "# .to(device)\n",
    "draft_generator = pipeline('text-generation', model=draft_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
    "# .to(device)\n",
    "target_generator = pipeline('text-generation', model=target_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'The quick brown fox'\n",
    "inputs = tokenizer(question, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = speculative_sampling(target_model, draft_model, initial_prompt_seq=inputs.input_ids, max_new_tokens= 20, lookahead=4, tokenizer=tokenizer, temperature=0., debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokens = len(tokens[0]) - len(inputs.input_ids[0])\n",
    "print(new_tokens)\n",
    "print(tokenizer.decode(tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = autoregressive_sampling(target_model, initial_prompt_seq=inputs.input_ids, max_new_tokens=11, temperature=0.)\n",
    "new_tokens = len(tokens[0]) - len(inputs.input_ids[0])\n",
    "print(new_tokens)\n",
    "print(tokenizer.decode(tokens[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'United States in the year 2025'\n",
    "temperature = 0\n",
    "max_lengths = [32, 64, 128, 256]  # example max_lengths\n",
    "lookahead_ks = [1, 2, 3, 4, 5, 7, 8]  # exaample lookahead_ks\n",
    "\n",
    "# Store the results\n",
    "results = []\n",
    "\n",
    "# Loop over different max_length and k values\n",
    "for max_length in max_lengths:\n",
    "    # Run autoregressive sampling\n",
    "    output_ar, time_ar, new_tokens_ar = sampling_test(prompt, tokenizer, 'autoregressive', target_model, draft_model, max_new_tokens=max_length)\n",
    "    \n",
    "    results.append(['autoregressive', max_length, 0, time_ar, output_ar, new_tokens_ar])\n",
    "    \n",
    "    for k in lookahead_ks:\n",
    "        # Run speculative sampling\n",
    "        output_sp, time_sp, new_tokens_sp = sampling_test(prompt, tokenizer, 'speculative', target_model, draft_model, max_new_tokens=max_length, lookahead_k=k)\n",
    "\n",
    "        results.append(['speculative', max_length, k, time_sp, output_sp, new_tokens_sp])\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(results, columns=['Sampling Method', 'Max Length', 'K Values', 'Time Taken', 'Text Generated', 'New Tokens'])\n",
    "\n",
    "# Save the results to a CSV file\n",
    "csv_file_path = f'{model_names}-sampling-times-temp{temperature}.csv'\n",
    "df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the unique N values (Max Length) and K values for lookahead\n",
    "max_lengths_sorted = sorted(df['Max Length'].unique())\n",
    "lookahead_ks = sorted(df[df['Sampling Method'] == 'speculative']['K Values'].unique())\n",
    "ind = np.arange(len(max_lengths_sorted))\n",
    "bar_width = 0.1\n",
    "\n",
    "\n",
    "# Adjust the plot aesthetics as per the user's request\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Set the color for the bars\n",
    "color_speculative = 'lightgreen'  # A more pleasant color for speculative bars\n",
    "color_autoregressive = 'lightblue'  # A more pleasant color for autoregressive bars\n",
    "\n",
    "# Iterate over max_lengths and k_values to create bar positions\n",
    "for i, max_length in enumerate(max_lengths_sorted):\n",
    "    # Select subset of DataFrame for the current N (Max Length)\n",
    "    subset = df[df['Max Length'] == max_length]\n",
    "    \n",
    "    # Plot bars for autoregressive method\n",
    "    autoregressive_time = subset[subset['Sampling Method'] == 'autoregressive']['Time Taken'].values\n",
    "    if autoregressive_time.size > 0:\n",
    "        ax.bar(ind[i] - bar_width/2, autoregressive_time, bar_width, label='Autoregressive' if i == 0 else \"\", \n",
    "               color=color_autoregressive, edgecolor='black')\n",
    "    \n",
    "    # Plot bars for speculative method with different K values\n",
    "    speculative_subset = subset[subset['Sampling Method'] == 'speculative']\n",
    "    for j, k_value in enumerate(speculative_subset['K Values']):\n",
    "        time = speculative_subset[speculative_subset['K Values'] == k_value]['Time Taken'].values\n",
    "        if time.size > 0:\n",
    "            bar = ax.bar(ind[i] + (j+0.5)*bar_width, time, bar_width, label=f'Speculative K' if i == 0 else \"\", \n",
    "                         color=color_speculative, edgecolor='black')\n",
    "            # Annotate K value on the bar\n",
    "            ax.annotate(f'K={k_value}',\n",
    "                        xy=(bar[0].get_x() + bar[0].get_width() / 2, bar[0].get_height()),\n",
    "                        xytext=(0, 3),  # 3 points vertical offset\n",
    "                        textcoords=\"offset points\",\n",
    "                        ha='center', va='bottom')\n",
    "\n",
    "# Add grid to the plot with reduced alpha for less bold lines\n",
    "ax.yaxis.grid(True, linestyle='--', which='major', color='grey', alpha=0.5)\n",
    "\n",
    "# Set the labels and legend\n",
    "ax.set_xticks(ind)\n",
    "ax.set_xticklabels(max_lengths_sorted)\n",
    "ax.set_xlabel('Max Length (N)')\n",
    "ax.set_ylabel('Time Taken (seconds)')\n",
    "ax.set_title(f'Time Taken for Different Sampling Methods and K Values (temp = {temperature})')\n",
    "ax.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot to a file\n",
    "plot_file_path = f'{model_names}-bar-plot-sampling-times-temp{temperature}.png'\n",
    "plt.savefig(plot_file_path)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lengths_sorted = sorted(df['Max Length'].unique())\n",
    "\n",
    "for max_length in max_lengths_sorted:\n",
    "    k = df[df['Max Length'] == max_length]['K Values']\n",
    "    time_list = df[df['Max Length'] == max_length]['Time Taken'] * 1000\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(k, time_list, label=f\"Max Length = {max_length}\", linestyle='--', marker='x')\n",
    "    plt.plot(k, time_list, 'x', color='red')\n",
    "\n",
    "    plt.grid(True, axis='y', linestyle='--', which='major', color='grey', alpha=0.5)\n",
    "    plt.xlabel('Number of draft tokens (K)')\n",
    "    plt.ylabel('Time Taken (m seconds)')\n",
    "    plt.title(f'Time Taken for K Values for max-length = {max_length} (temp = {temperature})')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{model_names}-time-taken-max-length-{max_length}.png')\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speculative_decoding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
