{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepMind Paper Implementation\n",
    "\n",
    "- delete models\n",
    "- huggingface-cli delete-cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://github.com/shreyansh26/Speculative-Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import numpy as np\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import time\n",
    "from typing import Tuple\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from transformers import set_seed\n",
    "import os \n",
    "import dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv('/Users/vashisth/Desktop/research-new/speculative-decoding/.env')\n",
    "hf_api = os.getenv('hf_api') \n",
    "print(os.getenv('hf_api') is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sampling(p):\n",
    "#     output = torch.multinomial(p, 1)\n",
    "#     return output.reshape(1,-1)\n",
    "\n",
    "# ' x is the tokenized prompt, max_new_tokens = N'\n",
    "# def autoregressive_generation(x, model, max_new_tokens):\n",
    "#     n = len(x)\n",
    "#     N = max_new_tokens\n",
    "#     T = len(x) + N\n",
    "#     while n<T:\n",
    "#         logits = model_fn(model,x)\n",
    "#         output = sampling(logits)\n",
    "#         x = torch.cat((x, output), dim=-1)\n",
    "#         n += 1\n",
    "#     return x\n",
    "\n",
    "\n",
    "\n",
    "# (f)_+ function in the paper\n",
    "def max_fn(x): \n",
    "    x_max = torch.where(x > 0, x, 0)\n",
    "    x_max = x_max.float()\n",
    "    sum_ = torch.sum(x_max, dim = -1, keepdim=True) + 1e-8\n",
    "    x_max.div_( sum_)\n",
    "    return x_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distribution(logits, temperature, epsilon = 1e-8):\n",
    "    logits /= (temperature + epsilon)\n",
    "    probability = F.softmax(logits, dim = -1)\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(logits, temperature):\n",
    "    output = get_distribution(logits, temperature)\n",
    "    output = torch.multinomial(output, num_samples=1)\n",
    "    return output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_from_draft_model(model, initial_prompt_seq, new_tokens, temperature=1.0):\n",
    "    fin_prompt_seq = initial_prompt_seq.detach().clone()\n",
    "    out_logits = []\n",
    "\n",
    "    for _ in range(new_tokens):\n",
    "        sample_token_logits = model(fin_prompt_seq).logits[:, -1, :]\n",
    "        sample_token = sample(sample_token_logits, temperature=temperature)\n",
    "        fin_prompt_seq = torch.concat([fin_prompt_seq, sample_token.unsqueeze(0)], dim=-1)\n",
    "        out_logits.append(sample_token_logits)\n",
    "\n",
    "    out_logits = torch.stack(out_logits, dim=1)\n",
    "    return fin_prompt_seq, out_logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoregressive_sampling(model, initial_prompt_seq, max_new_tokens, temperature=1.0):\n",
    "    n = initial_prompt_seq.shape[-1]\n",
    "    target_len = n + max_new_tokens\n",
    "    fin_prompt_seq = initial_prompt_seq.detach().clone()\n",
    "\n",
    "    while n < target_len:\n",
    "        sample_token_logits = model(fin_prompt_seq).logits[:, -1, :]\n",
    "        sample_token = sample(sample_token_logits, temperature=temperature)\n",
    "        # fin_prompt_seq = torch.concat([fin_prompt_seq, sample_token[None,...]], dim=-1)\n",
    "        sample_token_unsqueezed = sample_token.unsqueeze(0) # to add batch dim \n",
    "        fin_prompt_seq = torch.concat([fin_prompt_seq, sample_token_unsqueezed], dim=-1)\n",
    "\n",
    "        n += 1\n",
    "    return fin_prompt_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speculative_sampling(target_model, draft_model, initial_prompt_seq, max_new_tokens, tokenizer, lookahead=3, temperature=1.0, debug=True):\n",
    "\n",
    "    assert initial_prompt_seq.shape[0] == 1, 'Batch size should be 1'\n",
    "\n",
    "    n = initial_prompt_seq.shape[-1]\n",
    "    target_len = n + max_new_tokens\n",
    "    fin_prompt_seq = initial_prompt_seq.detach().clone()\n",
    "\n",
    "    while n < target_len:\n",
    "        if debug:\n",
    "            print('____________________')\n",
    "            print('n: ', n)\n",
    "        n_orig = n\n",
    "        N = fin_prompt_seq.shape[-1]\n",
    "        draft_outputs, draft_logits = sample_from_draft_model(draft_model, fin_prompt_seq, new_tokens=lookahead, temperature=temperature)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"Possible continuations: {tokenizer.decode(draft_outputs[0,n_orig:], skip_special_tokens=True)}\")\n",
    "\n",
    "        target_logits = target_model(draft_outputs).logits[:, -lookahead-1:, :]\n",
    "\n",
    "        target_model_distribution = get_distribution(target_logits, temperature)\n",
    "        draft_model_distribution = get_distribution(draft_logits, temperature)\n",
    "\n",
    "        accepted_flag = True\n",
    "        \n",
    "        for t in range(lookahead):\n",
    "            numerator = target_model_distribution[:, t, draft_outputs[0, N+t]]\n",
    "            denominator = draft_model_distribution[:, t, draft_outputs[0, N+t]]\n",
    "            ratio = (numerator / denominator)\n",
    "            r = torch.rand_like(numerator) # Uniform[0]\n",
    "            ones_tensor = torch.ones_like(numerator)\n",
    "\n",
    "            # Rejection Sampling\n",
    "            ## Acceptance\n",
    "            if (r < torch.min(ones_tensor, ratio)).any():\n",
    "                fin_prompt_seq = torch.concat([fin_prompt_seq, draft_outputs[:, N+t].unsqueeze(dim=-1)], dim=-1)\n",
    "                n += 1\n",
    "                \n",
    "                if debug:\n",
    "                    accepted_token = tokenizer.decode(draft_outputs[0, N+t])\n",
    "                    print(f\"Accepted token: ''{accepted_token}'' \")\n",
    "\n",
    "            ## Rejection\n",
    "            else:\n",
    "                new_dist = (target_model_distribution[:, t, :] - draft_model_distribution[:, t, :])\n",
    "                new_dist = max_fn(new_dist)\n",
    "                token_id = torch.multinomial(new_dist, num_samples=1)[0]\n",
    "                fin_prompt_seq = torch.concat([fin_prompt_seq, token_id.unsqueeze(0) ], dim=-1)\n",
    "                \n",
    "                accepted_flag = False\n",
    "                \n",
    "                if debug:\n",
    "                    rejected_token = tokenizer.decode(draft_outputs[0, N+t])\n",
    "                    new_token = tokenizer.decode(token_id)\n",
    "                    print(f\"Rejected token: ''{rejected_token}'', Replaced with: {new_token}\")\n",
    "                break\n",
    "            \n",
    "            # Print full sentence after every token update\n",
    "        if debug:\n",
    "            full_sentence = tokenizer.decode(fin_prompt_seq[0], skip_special_tokens=True)\n",
    "            print(f\"Full sentence: {full_sentence}\")\n",
    "            \n",
    "        if accepted_flag:\n",
    "            sample_token = sample(target_logits[:, -1, :], temperature=temperature)\n",
    "            fin_prompt_seq = torch.concat([fin_prompt_seq, sample_token.unsqueeze(0) ], dim=-1)\n",
    "\n",
    "        if debug:\n",
    "            print(f\"Accepted continuations: {tokenizer.decode(fin_prompt_seq[0,n_orig:], skip_special_tokens=True)}\")\n",
    "\n",
    "        n += 1\n",
    "\n",
    "    return fin_prompt_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "draft_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "draft_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "draft_generator = pipeline('text-generation', model=draft_model, tokenizer=draft_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_tokenizer = AutoTokenizer.from_pretrained(\"gpt2-large\")\n",
    "target_model = AutoModelForCausalLM.from_pretrained(\"gpt2-large\")\n",
    "target_generator = pipeline('text-generation', model=target_model, tokenizer=target_tokenizer)\n",
    "tokenizer = target_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'the quick brown fox'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(question, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________\n",
      "n:  4\n",
      "Possible continuations: es, and\n",
      "Rejected token: ''es'', Replaced with:  jumps\n",
      "Full sentence: the quick brown fox jumps\n",
      "Accepted continuations:  jumps\n",
      "____________________\n",
      "n:  5\n",
      "Possible continuations:  up and down\n",
      "Rejected token: '' up'', Replaced with:  over\n",
      "Full sentence: the quick brown fox jumps over\n",
      "Accepted continuations:  over\n",
      "____________________\n",
      "n:  6\n",
      "Possible continuations:  the fence and\n",
      "Accepted token: '' the'' \n",
      "Rejected token: '' fence'', Replaced with:  lazy\n",
      "Full sentence: the quick brown fox jumps over the lazy\n",
      "Accepted continuations:  the lazy\n",
      "____________________\n",
      "n:  8\n",
      "Possible continuations: , lazy fox\n",
      "Rejected token: '','', Replaced with:  dog\n",
      "Full sentence: the quick brown fox jumps over the lazy dog\n",
      "Accepted continuations:  dog\n",
      "____________________\n",
      "n:  9\n",
      "Possible continuations:  and runs off\n",
      "Rejected token: '' and'', Replaced with: \"\n",
      "Full sentence: the quick brown fox jumps over the lazy dog\"\n",
      "Accepted continuations: \"\n",
      "____________________\n",
      "n:  10\n",
      "Possible continuations:  and \"the\n",
      "Rejected token: '' and'', Replaced with:  is\n",
      "Full sentence: the quick brown fox jumps over the lazy dog\" is\n",
      "Accepted continuations:  is\n",
      "____________________\n",
      "n:  11\n",
      "Possible continuations:  a common phrase\n",
      "Accepted token: '' a'' \n",
      "Accepted token: '' common'' \n",
      "Rejected token: '' phrase'', Replaced with:  saying\n",
      "Full sentence: the quick brown fox jumps over the lazy dog\" is a common saying\n",
      "Accepted continuations:  a common saying\n",
      "____________________\n",
      "n:  14\n",
      "Possible continuations:  in the dog\n",
      "Rejected token: '' in'', Replaced with: .\n",
      "Full sentence: the quick brown fox jumps over the lazy dog\" is a common saying.\n",
      "Accepted continuations: .\n"
     ]
    }
   ],
   "source": [
    "tokens = speculative_sampling(target_model, draft_model, initial_prompt_seq=inputs.input_ids, max_new_tokens= 11, lookahead=3, tokenizer=tokenizer, temperature=0., debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "the quick brown fox jumps over the lazy dog\" is a common saying.\n"
     ]
    }
   ],
   "source": [
    "new_tokens = len(tokens[0]) - len(inputs.input_ids[0])\n",
    "print(new_tokens)\n",
    "print(tokenizer.decode(tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "the quick brown fox jumps over the lazy dog\" is a common saying.\n"
     ]
    }
   ],
   "source": [
    "tokens = autoregressive_sampling(target_model, initial_prompt_seq=inputs.input_ids, max_new_tokens=11, temperature=0.)\n",
    "\n",
    "new_tokens = len(tokens[0]) - len(inputs.input_ids[0])\n",
    "print(new_tokens)\n",
    "print(tokenizer.decode(tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speculative_decoding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
